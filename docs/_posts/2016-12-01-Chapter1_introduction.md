---
title: 前言
layout: post
share: false
---
远在古希腊时期，创造者就梦想着创造能思考的机器。
神话人物皮格马利翁(Pygmalion)、代达罗斯(Daedalus)和赫淮斯托斯(Hephaestus)都可以被看作传说中的造物者，而加拉蒂亚(Galatea)、塔洛斯(Talos)和潘多拉(Pandora)都可以被视为人造生命。

当人类第一次构思可编程计算机时，就已经在思考计算机能否变得智能（尽管这距造出第一台还有一百多年）。
如今，\firstall{AI}是一个具有许多实际应用和活跃研究课题的领域，并蓬勃发展着。
我们指望通过智能软件自动化处理常规劳动、理解语音或图像、帮助医学诊断和支持基础科学研究。

在\gls{AI}的早期，那些对人类智力来说非常困难但对计算机来说相对简单的问题得到迅速解决，比如那些可以通过一系列形式的数学规则来描述的问题。
\gls{AI}的真正挑战被证明是解决对人来说很容易执行，但很难形式化描述的任务，也就是我们人类能自动的靠直观解决的问题，比如识别所说的话或图像中的脸。

这本书讨论这些更直观的问题一种解决方案。
这种解决方案是为了让计算机从经验中学习，并通过层次化概念体系来理解世界，其中每个概念通过与较简单概念之间的联系来定义。
让计算机通过经验获取知识，就不需要人类来形式化地列举计算机需要的所有知识。
层次化的概念让计算机构建较简单的概念来学习复杂概念。
如果绘制出这些概念如何建立在彼此之上的图，我们将得到一张"深"（层次很多）的图。
出于这个原因，我们称这种方法为\glssymbol{AI}\firstgls{DL}。

<!-- % -- 1 -- -->

许多\glssymbol{AI}的早期成功发生在相对干净且形式的环境中， 计算机不需要具备很多关于世界的知识。
例如，IBM的深蓝（Deep Blue）国际象棋系统在1997年击败了世界冠军\ENNAME{Garry Kasparov}。
当然国际象棋是一个非常简单的领域，仅含有64个位置并只能以严格限制的方式移动32个棋子。
设计一种成功的国际象棋策略是巨大的成就，但挑战并不是向计算机描述棋子和允许的移动的困难性。
国际象棋完全可以由一个非常简短的、完全形式化的规则列表描述，并可以轻松由程序员提前提供。

讽刺的是，抽象和形式的任务对人类而言是最困难的脑力任务之一，对计算机而言却属于最容易的。
计算机早已能够打败即便是最好的人类棋手，但直到最近才在识别对象或语音的任务中到达匹配人类平均的能力。
一个人的日常生活需要关于世界的巨量知识。
很多这方面的知识是主观的、直观的，因此很难通过形式的方式表达清楚。
为了表现出智能，计算机需要获取同样的知识。
\gls{AI}的一个关键挑战就是如何将这些非形式的知识传达给计算机。

一些\gls{AI}项目都力求将关于世界的知识用形式化的语言进行硬编码。
计算机可以通过这些形式化语言自动地使用逻辑推理规则来理解声明。
这就是所谓的\gls{AI}的\firstgls{knowledge_base}方法。
这些项目都没有导致重大的成功。
其中最著名的项目是的Cyc 。
Cyc包括一个\gls{inference}引擎和一个使用CycL语言描述的声明数据库。
这些声明是由人类监督者输入的。
这是一个笨拙的过程。
人们设法设计出足够复杂的、能准确描述世界的形式规则。
例如，Cyc不能理解一个关于名为\ENNAME{Fred}的人在早上剃须的故事。
它的推理引擎检测到故事中的不一致性：它知道人没有电气零件，但由于\ENNAME{Fred}拿着一个电动剃须刀，它认为实体"FredWhileShaving"含有电气部件。
因此就会产生这样的疑问——\ENNAME{Fred}在刮胡子的时候是否仍然是一个人。

依靠硬编码的知识体系面对的困难表明，\glssymbol{AI}系统需要具备自己获取知识的能力，即从原始数据中提取模式的能力。
这种能力被称为\firstgls{ML}。
引入\gls{ML}使计算机能够解决涉及现实世界知识的问题，并能作出看似主观的决策。
所谓\firstgls{logistic_regression}的简单\gls{ML}算法可以决定是否建议剖腹产。
所谓\firstgls{naive_bayes}的简单\gls{ML}算法可以区分垃圾电子邮件和合法电子邮件。

<!-- % -- 2 -- -->

这些简单的\gls{ML}算法的性能在很大程度上依赖于给定数据的\firstgls{representation}。
例如，当\gls{logistic_regression}被用于推荐剖腹产时，\glssymbol{AI}系统不直接检查患者。
相反，需要医生告诉系统几条相关的信息，诸如子宫疤痕是否存在。
表示患者的每条信息被称为一个特征。
\gls{logistic_regression}学习病人的这些特征如何与各种结果相关联。
然而，它丝毫不能影响该特征定义的方式。
如果将病人的MRI扫描作为\gls{logistic_regression}的输入，而不是医生正式的报告，它将无法作出有用的预测。
MRI扫描的单一像素与分娩过程中的并发症只有微不足道的相关性。

对\gls{representation}的依赖是在整个计算机科学乃至日常生活中出现的普遍现象。
在计算机科学中，如果数据集合经过精巧的结构化并建立索引，数据操作的处理速度可以成倍的加快（如搜索）。
人们可以很容易地在阿拉伯数字的\gls{representation}下进行算术运算，但在罗马数字的\gls{representation}下运算会更耗时。
毫不奇怪，\gls{representation}的选择会对机器学习算法的性能产生巨大的影响。
\fig?显示了一个简单的可视化例子。
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter1/figures/polar_color}}
\fi
\caption{不同表示的例子：假设我们想在散点图中画一条线来分隔两类数据。
在左图，我们使用笛卡尔坐标表示数据，这个任务是不可能的。 
右图中，我们用极坐标表示数据，可以用垂直线简单地解决这个任务。（与David Warde-Farley合作画出此图。）}
\end{figure}

许多\gls{AI}的任务都可以先提取一个合适的特征集，然后将这些特征提供给简单的\gls{ML}算法来解决。
例如，从声音鉴别说话者的一个有用特征是说话者声道大小的估计。
这个特征为判断说话者是一个男性，女性还是儿童提供了有力线索。

然而，对于许多任务来说，很难知道应该提取哪些特征。
例如，假设我们想编写一个程序来检测照片中的车。
我们知道，汽车有轮子，所以我们可能会想用车轮的存在作为特征。
不幸的是，我们难以准确地从像素值的角度描述一个车轮看起来如何。
车轮具有简单的几何形状，但它的图像可以因为环境而变得很复杂，如落在车轮上的阴影、太阳照亮的车轮的金属零件、汽车的挡泥板或者遮挡的车轮一部分的前景物体等等。

<!-- % -- 3 -- -->

解决这个问题一个途径的是使用\gls{ML}来发现\gls{representation}本身，而不仅仅把\gls{representation}映射到输出。
这种方法被称为\firstgls{representation_learning}。
学习到的\gls{representation}往往比手动设计的\gls{representation}表现得更好。
并且它们只需最少的人工干预，就能让\glssymbol{AI}系统迅速适应新的任务。
\gls{representation_learning}算法只需几分钟就可以为简单的任务发现一个很好的特征集，对于复杂任务则需要几小时到几个月。
手动为一个复杂的任务设计特征需要耗费大量的人工时间和精力；甚至需要花费整个社区研究人员几十年的时间。

\gls{representation_learning}算法的典型例子是\firstgls{AE}。
\gls{AE}组合了将输入转换到不同\gls{representation}\firstgls{encoder}函数和将新的\gls{representation}转回原来形式的\firstgls{decoder}函数。 
\gls{AE}的训练目标是，输入经过\gls{encoder}和\gls{decoder}之后尽可能多的保留信息，同时希望新的\gls{representation}有各种好的属性。不同种类的\gls{AE}的目标是实现不同种类的属性。

当设计特征或学习特征的算法时，我们的目标通常是分离出能解释观察数据的\textbf{变化因素}(factors of variation)。
在此背景下，"因素"这个词仅指代影响的不同来源；因素通常不是乘性组合。
这些因素通常是不能被直接观察到的量。
相反，它们可能是现实世界中观察不到的物体或者不可观测的力，但会影响能观测到的量。
为了给观察到的数据提供有用的简化解释或推断原因，它们还存在于人类的思维构造中。
它们可以被看作数据的概念或者抽象，帮助我们了解这些富有变化的数据。
当分析语音记录时，变化的因素包括说话者的年龄、性别、他们的口音和他们正在说的词语。
当分析汽车的图像时，变化的因素包括汽车的位置、它的颜色、太阳的角度和亮度。

<!-- % -- 4 -- -->

在现实世界，许多\gls{AI}应用困难的一个重要原因是很多变化因素影响着我们能够观察到的每一个数据。
在夜间，一张图片中红色汽车的单个像素可能会非常接近黑色。
汽车轮廓的形状取决于视角。
大多数应用需要我们\emph{理清}变化因素并丢弃我们不关心的因素。

当然，从原始数据中提取这样高层次的、抽象的特征是非常困难的。
许多这样的变化因素，诸如说话者的口音，只能对数据进行复杂的、接近人类水平的理解来确定。
它几乎与获得原来问题的\gls{representation}一样困难，乍一看，\gls{representation_learning}似乎并不能帮助我们。

\firstgls{DL}通过其它较简单的\gls{representation}来表达复杂\gls{representation}，解决了\gls{representation_learning}中的核心问题。

\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter1/figures/deep_learning}}
\fi
\caption{深度学习模型的示意图。 计算机难以理解原始感观输入数据的含义，如表示为像素值集合的图像。
将一组像素映射到对象标识的函数非常复杂。
如果直接处理，学习或评估此映射似乎是不可逾越的。
深度学习将所需的复杂映射分解为一系列嵌套的简单映射（每个由模型的不同层描述）来解决这一难题。
输入展示在\firstgls{visible_layer}，这样命名的原因是因为它包含我们能观察到的变量。
然后是一系列从图像中提取越来越多抽象特征的\firstgls{hidden_layer}。
因为它们的值不在数据中给出，所以将这些层称为"隐藏"; 模型必须确定哪些概念有利于解释观察数据中的关系。
这里的图像是每个\gls{hidden_unit}表示的特征的可视化。
给定像素，第一层可以比较相邻像素的亮度容易地识别边缘。
给定第一\gls{hidden_layer}中边的描述，第二\gls{hidden_layer}可以容易地搜索可识别为角和扩展轮廓的边集合。
给定第二\gls{hidden_layer}中关于角和轮廓的图像描述，第三\gls{hidden_layer}可以找到轮廓和角的特定集合来检测特定对象的整个部分。
最后，根据图像描述中包含的对象部分，可以识别图像中存在的对象。
经{ZeilerFergus14}许可转载此图。
}
\end{figure}

\gls{DL}让计算机通过较简单概念构建复杂的概念。
\fig?图像中一个人的概念。
\gls{DL}模型的典型例子是前馈深度网络或\firstall{MLP}。
\gls{MLP}仅仅是一个将一组输入值映射到输出的数学函数。
该函数由许多较简单的函数组合构成。
我们可以认为每个应用具有不同的数学函数，并为输入提供新的\gls{representation}。

学习数据正确\gls{representation}的想法是解释\gls{DL}的一个观点。
另一个观点是深度允许计算机学习一个多步骤的计算机程序。
\gls{representation}的每一层可以被认为是并行执行另一组指令后计算机的存储器状态。
更深的网络可以按顺序执行更多的指令。
顺序指令提供了极大的能力，因为后面的指令可以参考早期指令的结果。
根据这个观点，一层的激活函数没有必要对解释输入的变化因素进行编码。
\gls{representation}还存储着协助程序执行的状态信息，使输入更加有意义。
这里的状态信息类似于传统计算机程序中的计数器或指针。
它与具体的输入内容无关，但有助于模型组织其处理过程。

<!-- % -- 6 -- -->

目前主要有两种测量模型深度的方式。
第一观点是基于评估架构所需执行的顺序指令的数目。
我们可以认为这是描述每个给定输入后，计算模型输出的流程图的最长路径。
正如两个等价的计算机程序根据不同的语言将具有不同的长度，相同的函数可以被绘制为具有不同深度的流程图，这取决于我们允许使用的单步函数。
\fig?说明了语言的选择怎样给相同的架构两个不同的衡量。
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter1/figures/language}}
\fi
\caption{将输入映射到输出的计算图表的示意图，其中每个节点执行一个操作。
深度是从输入到输出的最长路径的长度，但这取决于可能的计算步骤的定义。
这些图中所示的计算是\gls{logistic_regression}模型的输出，$\sigma(\Vw^T \Vx)$，其中$\sigma$是\ENNAME{logistic sigmoid}函数。
如果我们使用加法、乘法和\ENNAME{logistic sigmoid}作为我们计算机语言的元素，那么这个模型深度为三。
如果我们将\gls{logistic_regression}视为元素本身，那么这个模型深度为一。
}
\end{figure}

另一种是在深度概率模型中使用的方法，不是将计算图的深度视为模型深度，而是将描述概念如何彼此相关的图的深度视为模型深度。
在这种情况下，计算每个概念\gls{representation}的计算流程图的深度可能比概念本身的图更深。
这是因为系统对较简单概念的理解可以在给出更复杂概念的信息后进一步细化。
例如，一个\glssymbol{AI}系统观察到其中一只眼睛在阴影中的脸部图像，最初可能只看到一只眼睛。
当检测到脸部的存在后，它可以推断第二只眼睛也可能是存在的。
在这种情况下，概念的图仅包括两层（关于眼睛的层和关于脸的层），但如果我们根据每个概念给出的其它$n$次估计进行细化，计算的图将包括$2n$个层。

<!-- % -- 7 -- -->

由于并不总是清楚计算图的深度或概率模型图的深度哪一个是最相关的，并且由于不同的人选择不同的最小元素集来构建相应的图，导致架构的深度不存在单一的正确值，就像计算机程序的长度不存在单一的正确值。
也不存在模型多么深才能被修饰为"深"的共识。
但相比传统\gls{ML} ，\gls{DL}研究的模型涉及更多学到函数或学到概念的组合，这点毋庸置疑。

总之， 这本书的主题——\gls{DL}是\glssymbol{AI}的途径之一。
具体来说，它是\gls{ML}的一种，一种允许计算机系统从经验和数据中得到提高的技术。
我们坚信\gls{ML}可以构建出在复杂实际环境下运行的\glssymbol{AI}系统，并且是唯一可行的方法。
\gls{DL}是一种特定类型的\gls{ML}，具有强大的能力和灵活性，它将大千世界\gls{representation}为嵌套的层次概念体系
（由较简单概念间的联系定义复杂概念、从一般抽象概括到高级抽象表示）。
\fig?展示了每个学科如何工作的高层次原理。
\begin{figure}[!hbt]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics[width=0.65\textwidth]{Chapter1/figures/venn}}
\fi
\caption{维恩图显示了深度学习是一种表示学习，也是一种机器学习，可以用于许多（但不是全部）\glssymbol{AI}方法。
维恩图的每个部分包括一个\glssymbol{AI}技术的示例。
}
\end{figure}

\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter1/figures/which_part_learned}}
\fi
\caption{流程图显示了\glssymbol{AI}系统的不同部分如何在不同的\glssymbol{AI}学科中彼此相关。
阴影框表示能从数据中学习的组件。}
\end{figure}


# 本书面向的读者


这本书对各类读者都有一定用处的，但我们是基于两个主要目标受众而写的。
其中一个目标受众是学习\gls{ML}的大学生（本科或研究生），包括那些已经开始职业生涯的\gls{DL}和\gls{AI}研究者。
另一个目标群体是没有\gls{ML}或统计背景但要迅速在他们的产品或平台中使用\gls{DL}的软件工程师。
\gls{DL}在许多软件领域都已被证明是有用的，包括计算机视觉、语音和音频处理、自然语言处理、机器人技术、生物信息学和化学、电子游戏、搜索引擎、网络广告和金融。

\begin{figure}[H]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics[width=0.65\textwidth]{Chapter1/figures/dependency}}
\fi
\caption{本书的高层组织。
从一章到另一章的箭头表示前一章是理解后一章的必备内容。}
\end{figure}

<!-- % -- 8 -- -->

为了最好地适应各类读者，这本书被组织为三个部分。
第一部分介绍基本的数学工具和\gls{ML}的概念。
第二部分介绍本质上已解决的技术、最成熟的\gls{DL}算法。
第三部分介绍被广泛认为是深度学习未来研究重点的但更具猜测性的想法。

读者可以随意跳过不感兴趣或与自己背景不相关的部分。
熟悉线性代数、概率和基本\gls{ML}概念的读者可以跳过第一部分，例如，当读者只是想实现一个能工作的系统则不需要阅读超出第二部分的内容。
为了帮助读者选择章节，\fig?展示了这本书的高层组织结构的流程图。

<!-- % -- 10 -- -->

我们假设所有读者都具备计算机科学背景。
也假设读者熟悉编程，并且对计算的性能问题、复杂性理论、入门级微积分和一些图论术语有基本的了解。
<!-- %  -->

# 深度学习的历史趋势

通过历史背景了解\gls{DL}是最简单的方式。
我们仅指出\gls{DL}的几个关键趋势，而不是提供详细的历史：

+ \gls{DL}有着悠久而丰富的历史，但随着很多反映不同哲学观点名称的尘封而渐渐消逝。
+ 随着可用的训练数据量不断增加，\gls{DL}变得更加有用。
+ 随着时间的推移，针对\gls{DL}的计算机软硬件基础设施都有所改善，\gls{DL}模型的规模也随之增长。
+ 随着时间的推移，\gls{DL}已经解决日益复杂的应用，并且精度不断提高。



## 神经网络的众多名称和命运变迁


我们期待这本书的许多读者都听说过\gls{DL}这一激动人心的新技术，并为一本书提及关于一个新兴领域的"历史"而感到惊讶。
事实上，\gls{DL}的历史可以追溯到20世纪40年代。
\gls{DL}只是\emph{看上去像}一个新的领域，因为在目前流行的前几年它是相对冷门的，同时也因为它被赋予了许多不同的已经消逝的名称，最近才成为所谓的"深度学习"。
这个领域已经更换了很多名称，反映了不同的研究人员和不同观点的影响。

讲述整个综合性的\gls{DL}历史超出了本书的范围。
然而，一些基本的背景对理解\gls{DL}是有用的。
一般来说，目前为止已经有三次\gls{DL}的发展浪潮：在20世纪40年代到60年代\gls{DL}被称为\firstgls{cybernetics}，20世纪80年代到90年代\gls{DL}被誉为\firstgls{connectionism}，并于2006年开始，以\gls{DL}之名复兴。
这在\fig?中定量给出。
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter1/figures/cybernetics_connectionism_ngrams_color}}
\fi
\caption{根据Google图书中短语"\gls{cybernetics}"、"\gls{connectionism}"或"\gls{NN}"频率衡量的\gls{ANN}研究的历史浪潮（图中显示了三次浪潮的前两次，第三次最近才出现）。
第一次浪潮开始于20世纪40年代到20世纪60年代的\gls{cybernetics}，随着生物学习理论的发展
和第一个模型的实现（如感知机~） ，能实现单个神经元的训练。
第二次浪潮开始于1980-1995年间的\gls{connectionism}方法，可以使用反向传播 训练具有一两个隐藏层的神经网络。
当前第三次浪潮，也就是深度学习，大约始于2006年，并且现在在2016年以书的形式出现。
另外两次浪潮类似地出现在书中的时间比相应的科学活动晚得多。
}
\end{figure}

<!-- % -- 12 -- -->

我们现在认识的一些最早的学习算法，旨在模拟生物学习的计算模型，即大脑怎样学习或为什么能学习的模型。
其结果是，已消逝的\gls{DL}的名称之一——\firstall{ANN}。
此时\gls{DL}模型对应的观点是他们设计的系统是受生物大脑（无论人类大脑或其他动物的大脑）所启发。
尽管有些\gls{ML}的\gls{NN}有时被用来理解大脑功能，它们一般都没有被设计成生物功能的真实模型。
\gls{DL}的神经观点受两个主要思想启发的。
一个想法是，大脑这个例子证明智能行为的可能性，因此建立智能概念上的直接途径是逆向大脑背后的计算原理，并复制其功能。
另一种看法是，理解大脑和人类智力背后的原则也非常有趣，因此\gls{ML}模型除了解决工程应用的能力， 如果能阐明这些基本的科学问题也将会很有用。 

<!-- % -- 13 -- -->
  
现代术语"\gls{DL}"超越了目前\gls{ML}模型的神经科学观点。
学习的\emph{多层次组合}这一更普遍的原则更加吸引人，这可以应用于\gls{ML}框架且不必是受神经启发的。
 
 
现代\gls{DL}的最早前身是从神经科学的角度出发的简单线性模型。
这些模型被设计为使用一组$\Sn$个输入$\Sx_1, \dots ,\Sx_n$并将它们与一个输出$\Sy$相关联。 
这些模型将学习一组权重$\Sw_1, \dots, \Sw_n $ 并计算它们的输出$f(\Vx, \Vw) = \Sx_1 \Sw_1 + \dots + \Sx_n \Sw_n$。
这第一次\gls{NN}研究的浪潮被称为\gls{cybernetics}，如\fig?所示。

\ENNAME{McCulloch-Pitts}神经元是脑功能的早期模型。
该线性模型通过测试函数$f(\Vx,\Vw)$的是正还是负来识别两种不同类型的输入。
当然，为了使模型对应于期望的类别定义，需要正确地设置权重。
这些权重可以由操作人员设定。
在20世纪50年代，感知机来学习权重的模型。
约在同一时期，\textbf{自适应线性元件}(adaptive linear element, ADALINE)，简单地返回函数$f(\Vx)$本身的值来预测一个实数，并且还可以学习从数据预测这些数。

这些简单的学习算法大大影响了\gls{ML}的现代景象。
用于调节ADALINE权重的训练算法是称为\firstgls{SGD}的一种特例。
稍加修改的\gls{SGD}算法仍然是当今\gls{DL}的主要训练算法。

基于感知机和ADALINE中使用的函数$f(\Vx, \Vw)$的模型被称为\firstgls{linear_model}。
这些模型仍然是最广泛使用的\gls{ML}模型，尽管在许多情况下，它们以不同于原始模型的方式进行\emph{训练}。

\gls{linear_model}有很多局限性。
最著名的是，它们无法学习XOR函数，即$f([0,1], \Vw) = 1, f([1,0], \Vw)=1$，但$f([1,1], \Vw)=0, f([0,0],\Vw)= 0$。
在\gls{linear_model}中观察到这些缺陷的批评者开始反对受生物学启发的学习。
这是\gls{NN}第一次热度较多的下降。

现在，神经科学被视为\gls{DL}研究的一个重要灵感来源，但它已不再是该领域的主要导向。

<!-- % -- 14 -- -->

如今神经科学在\gls{DL}研究中的作用被削弱，主要原因是我们根本没有足够的关于大脑信息来作为指导。
要获得对大脑实际使用算法的深刻理解，我们需要有能力同时监测（至少是）数千相连神经元的活动。
我们不能够做到这一点，甚至连大脑的最简单、最深入研究的部分我们都还远远没有理解。

神经科学已经给了我们依靠单一\gls{DL}算法解决许多不同任务的理由。
神经学家们发现，如果将雪貂的大脑重新连接，使视觉信号传送到听觉区域，它们可以学会用大脑的听觉处理区域"看"。
这表明，多数哺乳动物大脑的可能使用单一的算法解决大部分大脑可以解决的不同任务。
这个假设之前，\gls{ML}研究更加分散，研究人员在不同的社区研究自然语言处理、计算机视觉、运动规划和语音识别。
如今，这些应用的社区仍然是独立的，但是\gls{DL}研究小组同时研究许多或甚至所有这些应用领域是很常见的。

我们能够从神经科学得到一些粗略的指南。
仅通过计算单元之间的相互作用而变得智能的基本思想是受大脑启发的。
新认知机看到。
目前大多数\gls{NN}基于称为\firstgls{ReLU}的神经单元模型。
原始认知机受我们关于大脑功能知识的启发， 引入了一个更复杂的版本。
简化的现代版基于许多观点进化发展，{Nair-2010}和{Glorot+al-AI-2011-small}援引神经科学作为影响，{Jarrett-ICCV2009}援引更多面向工程的影响。
虽然神经科学是灵感的重要来源，它不需要被视为刚性指导。
我们知道，实际的神经元与现代\gls{ReLU}计算着非常不同函数，但更接近真实神经网络的系统并没有导致\gls{ML}性能的提升。
此外，虽然神经科学已经成功地启发了一些\gls{NN}\emph{架构}，但我们还没有足够地了解生物学习的神经科学，因此在训练这些架构时，不能提供给我们很多关于\emph{学习算法}的指导。

媒体报道经常强调\gls{DL}与大脑的相似性。
虽然\gls{DL}研究人员更可能比其他\gls{ML}领域（如核机器或贝叶斯统计工作的研究人员）引用大脑作为影响，人们不应该认为\gls{DL}在尝试模拟大脑。
现代\gls{DL}从许多领域获取灵感，特别是应用数学的基本内容如线性代数、概率论、信息论和数值优化。
尽管一些\gls{DL}的研究人员引用神经科学作为灵感的重要来源，但其他学者完全不关心神经科学。

<!-- % -- 15 -- -->

值得注意的是，了解大脑是如何在算法层面上工作的尝试是鲜活且发展良好的。
这项尝试主要被称为"计算神经科学"，并且是独立于\gls{DL}的领域。
研究人员在两个领域之间反复研究是很常见的。
\gls{DL}领域主要关注如何构建智能的计算机系统，用来解决需要智能才能解决的任务，而计算神经科学领域主要关注构建大脑如何工作的更精确的模型。

在20世纪80年代，神经网络研究的第二次浪潮在很大程度上是伴随一个被称为\firstgls{connectionism}或\textbf{并行分布处理}( parallel distributed processing)运动而出现的。
\gls{connectionism}是在认知科学的背景下出现的。
认知科学是理解心智，并结合多个不同层次分析的跨学科方法。
在20世纪80年代初期，大多数认知科学家研究符号推理的模型。
尽管这很流行，但符号模型很难解释大脑如何真正使用神经元实现推理功能。 
连接主义者开始研究实际能基于神经实现的认知模型。

\gls{connectionism}的中心思想是，当网络将大量简单计算单元连接在一起时可以实现智能行为。
这种见解同样适用于与计算模型中隐藏单元作用类似的生物神经系统中的神经元。  

上世纪80年代的\gls{connectionism}运动过程中形成的几个关键概念在今天的\gls{DL}中仍然是非常重要的。

其中一个概念是\firstgls{distributed_representation}。
这一想法是系统每个的输入应该由许多特征\gls{representation}的，并且每个特征应参与许多可能输入的\gls{representation}。
例如，假设我们有一个能够识别红色、绿色、或蓝色的汽车、卡车和鸟类的视觉系统。
\gls{representation}这些输入的其中一个方法是将九个可能的组合：红卡车，红汽车，红鸟，绿卡车等等使用单独的神经元或\gls{hidden_unit}激活。
这需要九个不同的神经元，并且每个神经必须独立地学习颜色和对象身份的概念。
改善这种情况的方法之一是使用\gls{distributed_representation}，即用三个神经元描述颜色，三个神经元描述对象身份。 
这仅仅需要6个神经元而不是9个，并且描述红色的神经元能够从汽车、卡车和鸟类的图像中学习红色，而不仅仅是从一个特定类别的图像中学习。 
\gls{distributed_representation}的概念是本书的核心，我们将在\chap?中更加详细地描述。

<!-- % -- 16 -- -->

\gls{connectionism}运动的另一个重要成就是反向传播算法的成功运用（训练具有内部\gls{representation}的深度\gls{NN}）和普及。
这个算法虽然曾黯然失色不再流行，但截至写书之时，仍是训练深度模型的主要方法。% ??

在20世纪90年代，研究人员在使用\gls{NN}进行序列建模的方面取得了重要进展。
{Hochreiter91}和{Bengio1994ITNN}指出了建模长序列的一些根本数学难题，这将在\sec?中描述。
{Hochreiter+Schmidhuber-1997}引入\firstall{LSTM}网络来解决这些难题。
如今，\glssymbol{LSTM}在许多序列建模任务中广泛应用，包括Google的许多自然语言处理任务。

\gls{NN}研究的第二次浪潮一直持续到上世纪90年代中期。
基于\gls{NN}等其他\glssymbol{AI}技术的企业开始在寻求投资的同时，做不切实际野心勃勃的主张。
当\glssymbol{AI}研究不能实现这些不合理的期望时，投资者感到失望。
同时，\gls{ML}的其他领域取得进步。
核学习机都在很多重要任务上实现了很好的效果。
这两个因素导致了\gls{NN}热度的第二次下降，一直持续到2007年。

在此期间，\gls{NN}持续在某些任务上获得令人印象深刻的表现。
加拿大高级研究所（CIFAR）通过其神经计算和自适应感知（NCAP）研究计划帮助维持\gls{NN}研究。
这个计划统一了分别由\ENNAME{Geoffrey Hinton}，\ENNAME{Yoshua Bengio}和\ENNAME{Yann LeCun}引导的多伦多大学、蒙特利尔大学和纽约大学的\gls{ML}研究小组。
CIFAR NCAP研究计划具有多学科的性质，其中还包括人类神经科学家和计算机视觉专家。

<!-- % -- 17 -- -->

在那个时间点，普遍认为深度网络是难以训练的。
现在我们知道，20世纪80年代就存在的算法能工作得非常好，但是直到在2006年前后都没有体现出来。
这个问题可能是单纯的因为计算复杂性太高，而以当时可用的硬件难以进行足够的实验。

神经网络研究的第三次浪潮始于2006年的突破。
\ENNAME{Geoffrey Hinton}表明名为\gls{DBN}的\gls{NN}可以使用一种称为贪心逐层训练的策略进行有效地训练中更详细地描述。
CIFAR附属的其他研究小组很快表明，同样的策略可以被用来训练许多其他类型的深度网络，并能系统地帮助提高在测试样例上的泛化能力。
\gls{NN}研究的这一次浪潮普及了"\gls{DL}"这一术语的使用，强调研究人员现在可以训练以前不可能训练的更深的神经网络，并把注意力集中于深度的理论意义。
此时，深度\gls{NN}已经优于与之竞争的基于其他\gls{ML}技术以及手工设计函数的\glssymbol{AI}系统。
神经网络流行的第三次浪潮在写这本书的时候还在继续，尽管深度学习的研究重点在这一段时间内发生了巨大变化。
第三次浪潮开始把重点放在新的无监督学习技术和从小数据集进行泛化的能力，但目前更多的注意点是在更古老的监督学习算法和深度模型充分利用大型标注数据集的能力。


## 与日俱增的数据量

人们可能想问，尽管人工\gls{NN}的第一个实验在20世纪50年代就完成了，为什么\gls{DL}直到最近才被认为是关键技术。
自20世纪90年代以来，\gls{DL}就已经成功用于商业应用，但通常被视为是一种艺术而不是一种技术，且只有专家可以使用的艺术，这种观点持续到最近。
确实，要从一个\gls{DL}算法获得良好的性能需要一些技巧。
幸运的是，随着训练数据的增加，所需的技巧正在减少。
目前在复杂的任务达到与人类表现的学习算法，与20世纪80年代努力解决的玩具问题(toy problem)的学习算法几乎是一样的，尽管这些算法训练的模型经历了变革，简化了极深架构的训练。
最重要的新进展是现在我们有了这些算法成功训练所需的资源。
\fig?展示了基准数据集的大小如何随着时间的推移显著增加。
这种趋势是由社会日益数字化驱动的。
由于我们的活动越来越多发生在计算机上，我们做什么也越来越多地被记录。
我们的计算机越来越多地联网在一起，变得更容易集中管理这些记录，并将它们整理成适于\gls{ML}应用的数据集。
因为统计估计的主要负担（观察少量数据以在新数据上泛化）已经减轻，"大数据"的时代使\gls{ML}更加容易。
截至2016年，一个粗略的经验法则是，监督\gls{DL}算法一般在每类给定约5000标注样本情况下可以实现可接受的性能，当至少有1000万标注样本的数据集用于训练时将达到或超过人类表现。
在更小的数据集上成功是一个重要的研究领域，为此我们应特别侧重于如何通过无监督或半监督学习充分利用大量的未标注样本。
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter1/figures/dataset_size_color}}
\fi
\caption{与日俱增的数据量。
20世纪初，统计学家使用数百或数千的手动制作的测量来研究数据集。
20世纪50年代到80年代，受生物启发的机器学习开拓者通常使用小的合成数据集，如低分辨率的字母位图，设计为在低计算成本下表明神经网络能够学习特定功能。
20世纪80年代和90年代，机器学习变得更加统计，并开始利用包含成千上万个样本的更大数据集，如手写扫描数字的MNIST数据集（如\fig?）所示。
在21世纪初的第一个十年，相同大小更复杂的数据集持续出现，如CIFAR-10数据集 。
在这十年结束和下五年，明显更大的数据集（包含数万到数千万的样例）完全改变了深度学习的可能实现的事。
这些数据集包括公共Street View House Numbers数据集 。
在图顶部，我们看到翻译句子的数据集通常远大于其他数据集，如根据Canadian Hansard制作的IBM数据集 。
}
\end{figure}
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics[width=0.8\textwidth]{Chapter1/figures/mnist}}
\fi
\caption{MNIST数据集的输入样例。
"NIST"代表国家标准和技术研究所(National Institute of Standards and Technology)，是最初收集这些数据的机构。
"M"代表"修改的(Modified)"，为更容易地与机器学习算法一起使用，数据已经过预处理。
MNIST数据集包括手写数字的扫描和相关标签（描述每个图像中包含0-9中哪个数字）。
这个简单的分类问题是深度学习研究中最简单和最广泛使用的测试之一。
尽管现代技术很容易解决这个问题，它仍然很受欢迎。
Geoffrey Hinton将其描述为"机器学习的\emph{果蝇}"，这意味着机器学习研究人员可以在受控的实验室条件下研究他们的算法，就像生物学家经常研究果蝇一样。
}
\end{figure}

<!-- % -- 20 -- -->


## 与日俱增的模型规模


相对20世纪80年代较少的成功，现在\gls{NN}非常成功的另一个重要原因是我们现在拥有的计算资源可以运行更大的模型。
\gls{connectionism}的主要见解之一是，当动物的许多神经元一起工作时会变得聪明。
单独神经元或小集合的神经元不是特别有用。

生物神经元没有连接的特别密集。
如\fig?模型中每个神经元的连接数量甚至与哺乳动物的大脑在同一数量级。
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter1/figures/number_of_synapses_color}}
\fi
\caption{与日俱增的每神经元连接数。 % ？ 可以翻成平均吗 ？
最初，\gls{ANN}中神经元之间的连接数受限于硬件能力。
而现在，神经元之间的连接数大多是出于设计考虑。
一些\gls{ANN}中每个神经元的连接数与猫一样多，并且对于其他神经网络来说，每个神经元的连接与较小哺乳动物（如小鼠）一样多是非常普遍的。
甚至人类大脑每个神经元的连接也没有过高的数量。
生物神经网络规模来自{number_of_neurons}。
}
{\tiny
\begin{enumerate}
+sep0em
+ % 1
    自适应线性单元~
+ % 2
    神经认知机~
+ % 3
    GPU-加速 \gls{convolutional_network}~
+ % 4
    \gls{DBM}~
+ % 5
    \gls{unsupervised}\gls{convolutional_network}~
+ % 6
    GPU-加速 \gls{MLP}~
+ % 7
    分布式\gls{AE}~
+ % 8
    Multi-GPU \gls{convolutional_network}~
+ % 9
    COTS HPC  \gls{unsupervised}\gls{convolutional_network}~
+ % 10
    GoogLeNet~
\end{enumerate}
} % end tiny
\end{figure}

如\fig?都是惊人的少。
自从引入\gls{hidden_unit}以来，人工\gls{NN}的规模大约每2.4年扩大一倍。
这种增长是由更大内存、更快的计算机和更大的可用数据集驱动的。
较大的网络能够在更复杂的任务中实现更高的精度。
这种趋势看起来将持续数十年。
除非有允许迅速扩展的新技术，否则至少直到21世纪50年代，人工\gls{NN}将才能具备与人脑相同数量级的神经元。
生物神经元表示的函数可能比目前的人工神经元更复杂，因此生物神经网络可能比图中描绘的更大。
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter1/figures/number_of_neurons_color}}
\fi
\caption{与日俱增的神经网络规模。
自从引入\gls{hidden_unit}，\gls{ANN}的大小大约每2.4年翻一倍。
生物神经网络规模来自{number_of_neurons}。
}
{\tiny
\begin{enumerate}
+sep-.1em
+ % 1
    感知机~
+ % 2
    自适应线性单元~
+ % 3
    神经认知机~
+ % 4
    早期后向传播网络~
+ % 5
    用于语音识别的\gls{RNN}~
+ % 6
    用于语音识别的\gls{MLP}~
+ % 7
    \gls{meanfield}sigmoid\gls{BN}~
+ % 8
    LeNet-5~
+ % 9
    \gls{ESN}~
+ % 10
    \gls{DBN}~
+ % 11
    GPU-加速\gls{convolutional_network}~
+ % 12
    \gls{DBM}~
+ % 13
    GPU-加速\gls{DBN}~
+ % 14
    \gls{unsupervised}\gls{convolutional_network}~
+ % 15
    GPU-加速\gls{MLP}~
+ % 16
    OMP-1 网络~
+ % 17
    分布式\gls{AE}~
+ % 18
    Multi-GPU\gls{convolutional_network}~
+ % 19
    COTS HPC \gls{unsupervised}\gls{convolutional_network}~
+ % 20
    GoogLeNet~
\end{enumerate}
}
\end{figure}


现在回想起来，比一个水蛭的神经元还少的\gls{NN}不能解决复杂的\gls{AI}问题是不足为奇的。
即使现在从一个计算系统角度来看可能相当大的网络， 实际上比相对原始的脊椎动物如青蛙的神经系统更小。

由于更快的CPU、通用GPU的到来（在\sec?历史中最重要的趋势之一。
普遍预计这种趋势将很好地持续到未来。

<!-- % -- 21 -- -->


## 与日俱增的精度、复杂度和对现实世界的冲击


20世纪80年代以来，\gls{DL}提供精确识别和预测的能力一直在提高。
此外，\gls{DL}持续成功地应用于越来越广泛的应用。

最早的深度模型被用来识别裁剪得很合适且非常小的图像中的单个对象。
自那时以来，\gls{NN}可以处理的图像尺寸逐渐增加。
现代对象识别网络能处理丰富的高分辨率照片，并且不要求在被识别的对象附近进行裁剪。
类似地，最早网络只能识别两种对象（或在某些情况下，单一种类的对象的存在与否），而这些现代网络通常识别至少\NUMTEXT{1000}个不同类别的对象。
对象识别中最大的比赛是每年举行的ImageNet大型视觉识别挑战（ILSVRC）。
\gls{DL}迅速崛起的一个戏剧性时刻是卷积网络第一次大幅赢得这一挑战，将最高水准的前5错误率从\NUMTEXT{26.1\%}降到\NUMTEXT{15.3\%}的测试样例的正确类别不会出现在此列表中的前5。
此后，深度卷积网络一直能在这些比赛中取胜，截至写书时，\gls{DL}的进步将这个比赛中的前5错误率降到\NUMTEXT{3.6\%}， 如\fig?所示。
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter1/figures/imagenet_color}}
\fi
\caption{日益降低的错误率。
由于深度网络达到了在ImageNet大规模视觉识别挑战中竞争所必需的规模，他们每年都能赢得胜利，并且产生越来越低的错误率。
数据来源于 {russakovsky2014imagenet}和{He-et-al-arxiv2015}。}
\end{figure}

<!-- % -- 23 -- -->

\gls{DL}也对语音识别产生了巨大影响。
语音识别在20世纪90年代提高后，直到约2000年都停滞不前。
\gls{DL}的引入导致语音识别错误率陡然下降，有些错误率甚至降低了一半。 
我们将在\sec?更详细地探讨这个历史。

深度网络在行人检测和图像分割中也有引人注目的成功。

在深度网络的规模和的精度有所提高的同时，它们可以解决的任务也日益复杂。
{Goodfellow+et+al-ICLR2014a}表明，\gls{NN}可以学习输出描述图像的整个字符序列，而不是仅仅识别单个对象。
此前，人们普遍认为，这种学习需要对序列中的单个元素进行标注。
\gls{RNN}，如之前提到的\glssymbol{LSTM}序列模型，现在用于建模序列和其他序列之间的关系，而不是仅仅固定输入之间的关系。
这个序列到序列的学习似乎处于另一个应用演进的浪潮之巅：机器翻译。

<!-- % -- 24 -- -->

日益复杂的趋势已将其推向逻辑结论，即神经图灵机的引入，它能学习读取存储单元和向存储单元写入任意内容。
这样的\gls{NN}可以从期望行为的\gls{example}中学习简单的程序。
例如，从杂乱和排好序的\gls{example}中学习对一系列数进行排序。
这种自我编程技术正处于起步阶段，但原则上未来可以适用于几乎所有的任务。


\gls{DL}的另一个冠军成就是在\firstgls{RL}领域的扩展。
在\gls{RL}的背景下，一个自主体必须通过试错来学习执行任务，而无需人类操作者的任何指导。
DeepMind表明，基于\gls{DL}的\gls{RL}系统能够学会玩Atari视频游戏，并在多种任务中可与人类匹敌。
\gls{DL}也显著改善了机器人\gls{RL}的性能。

许多\gls{DL}应用都是高利润的。现在\gls{DL}被许多顶级的技术公司使用，包括Google、Microsoft、Facebook、IBM、Baidu、Apple、Adobe、Netflix、NVIDIA和NEC。

\gls{DL}的进步也严重依赖于软件基础架构的进展。
软件库如Theano都能支持重要的研究项目或商业产品。

\gls{DL}也为其他科学做出了贡献。
识别对象的现代卷积网络提供给神经科学家可以研究的视觉处理模型。
\gls{DL}也为处理海量数据、在科学领域作出有效的预测提供了非常有用的工具。
它已成功地用于预测分子如何相互作用，这能帮助制药公司设计新的药物。
我们期待\gls{DL}未来出现在越来越多的科学领域。

<!-- % -- 25 -- -->

总之，\gls{DL}是\gls{ML}的一种方法， 过去几十年的发展中，它深深地吸收了我们关于人脑、统计学与应用数学的知识。
近年来，\gls{DL}的普及性和实用性有了极大的发展，这在很大程度上得益于更强大的计算机、更大的数据集和能够训练更深网络的技术。
未来几年充满了进一步提高\gls{DL}并将它带到新领域的挑战和机遇。

<!-- % -- 26 -- -->
